{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvJyg8xgbBD2"
      },
      "source": [
        "Importación y Carga de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tJA_xfxpbAlp"
      },
      "outputs": [],
      "source": [
        "# Importaciones\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "df = pd.read_csv('Telco Churn dataset 2.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3B405XRbMm_"
      },
      "source": [
        "Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKuTS-4Da1iq",
        "outputId": "64269792-651b-47fc-a2aa-3a22c056507f"
      },
      "outputs": [],
      "source": [
        "# 1. Preprocesamiento\n",
        "# a. Crear la columna 'Churn' y asignar 1 si 'Churn' es 'Yes', de lo contrario 0\n",
        "df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "# b. Eliminar columnas con más del 50% de datos faltantes\n",
        "threshold = int(0.5 * len(df))\n",
        "df = df.dropna(thresh=threshold, axis=1)\n",
        "\n",
        "# c. Reemplazar valores atípicos por la media\n",
        "for col in df.select_dtypes(include=['float64', 'int64']):\n",
        "    if col != 'Churn':  # Asegurarse de no modificar la columna 'Churn'\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        df[col] = df[col].apply(lambda x: df[col].mean() if (x < (Q1 - 1.5 * IQR)) or (x > (Q3 + 1.5 * IQR)) else x)\n",
        "\n",
        "# d. Convertir variables categóricas a numéricas y llenar valores faltantes\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object' and column != 'Churn':\n",
        "        df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "        le = LabelEncoder()\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "    elif df[column].dtype in ['int64', 'float64']:\n",
        "        df[column].fillna(df[column].median(), inplace=True)\n",
        "\n",
        "# e. Normalización\n",
        "cols_to_scale = df.columns.tolist()\n",
        "cols_to_scale.remove('Churn')\n",
        "scaler = StandardScaler()\n",
        "df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
        "\n",
        "# f. Asegurarse de que 'Churn' sea int\n",
        "df['Churn'] = df['Churn'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GiFgAaybheu"
      },
      "source": [
        "Balanceo con SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Balanceo con SMOTE\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "smote = SMOTE(random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGkhCSKbZAy"
      },
      "source": [
        "División de ConjuntoBalanceo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pHoztCgybaO5"
      },
      "outputs": [],
      "source": [
        "# 4. División de Conjunto\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVuuaC5amVQU"
      },
      "source": [
        "Entrenamiento de ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyGhE6ygmYrG",
        "outputId": "1e0e50c0-ba48-4424-f724-6675dfddcaa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 24 Complete [00h 00m 03s]\n",
            "val_accuracy: 0.7785087823867798\n",
            "\n",
            "Best val_accuracy So Far: 0.906798243522644\n",
            "Total elapsed time: 00h 01m 16s\n",
            "(4560, 32)\n",
            "\n",
            "Los mejores hiperparámetros encontrados:\n",
            "Número de unidades: 48\n",
            "Tasa de aprendizaje: 0.01\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
            "  trackable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ],
      "source": [
        "# 5. Entrenamiento de ANN con búsqueda de hiperparámetros usando Keras Tuner\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units', min_value=8, max_value=64, step=8), \n",
        "                    input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(units=hp.Int('units', min_value=8, max_value=64, step=8), activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    optimizer = Adam(learning_rate=hp.Choice('learning_rate', values=[0.001, 0.01, 0.1]))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=100,\n",
        "    hyperband_iterations=2,\n",
        "    directory='my_dir',\n",
        "    project_name='keras_tuner_example'\n",
        ")\n",
        "\n",
        "# Ajusta el callback de ModelCheckpoint para guardar en el nuevo formato\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
        "    tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)\n",
        "]\n",
        "\n",
        "# Ejecutar la búsqueda de hiperparámetros\n",
        "tuner.search(X_train, y_train, epochs=100, validation_split=0.2, callbacks=callbacks)\n",
        "\n",
        "# Obtener los mejores hiperparámetros.\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(X_train.shape)  # Esto te mostrará las dimensiones actuales de entrada\n",
        "\n",
        "# Imprimir los mejores hiperparámetros\n",
        "print(f\"\"\"\n",
        "Los mejores hiperparámetros encontrados:\n",
        "Número de unidades: {best_hps.get('units')}\n",
        "Tasa de aprendizaje: {best_hps.get('learning_rate')}\n",
        "\"\"\")\n",
        "\n",
        "# Obtener el mejor modelo\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj8qFfB9b0t1"
      },
      "source": [
        "Evaluación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNJzas7gb1ND",
        "outputId": "89d1b5bf-f368-43c9-e735-73a6be8d09ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9017 - loss: 0.2521\n",
            "Loss: 0.2616870105266571, Accuracy: 0.8929824829101562\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "[[514  61]\n",
            " [ 61 504]]\n",
            "Reporte de clasificación:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89       575\n",
            "           1       0.89      0.89      0.89       565\n",
            "\n",
            "    accuracy                           0.89      1140\n",
            "   macro avg       0.89      0.89      0.89      1140\n",
            "weighted avg       0.89      0.89      0.89      1140\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 6. Evaluación del modelo con los mejores hiperparámetros\n",
        "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
        "\n",
        "# Obtener las predicciones en forma de clases\n",
        "y_pred = (best_model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calcular la matriz de confusión, y generar el reporte de clasificación\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Imprimir las métricas\n",
        "print(conf_matrix)\n",
        "print(\"Reporte de clasificación:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validación cruzada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy promedio con Validación Cruzada Manual: 0.9390350818634033\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# 7. Validación Cruzada\n",
        "# Asegúrate de tener los mejores hiperparámetros disponibles, 'best_hps', que deberían haber sido obtenidos de tu optimización con Keras Tuner\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "def create_best_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=best_hps.get('units'), input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(units=best_hps.get('units'), activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    optimizer = Adam(learning_rate=best_hps.get('learning_rate'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Configuración de KFold para la validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Lista para guardar las puntuaciones de cada fold\n",
        "accuracy_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X_train):\n",
        "    # Dividir datos en entrenamiento y prueba para este fold usando .iloc para DataFrames de pandas\n",
        "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "    # Crear y compilar el modelo\n",
        "    model = create_best_model()\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    model.fit(X_train_fold, y_train_fold, epochs=100, batch_size=10, verbose=0)\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de prueba del fold\n",
        "    _, accuracy = model.evaluate(X_test_fold, y_test_fold, verbose=0)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Calcular la media de las puntuaciones de precisión\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "print(f\"Accuracy promedio con Validación Cruzada Manual: {average_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
